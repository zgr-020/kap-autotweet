import os, re, json, hashlib, time
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import tweepy

FOREKS_URL = "https://www.foreks.com/analizler/piyasa-analizleri/sirket"
STATE_PATH = Path("data/posted.json")
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
)

TICKER_RE = re.compile(r"\b[A-ZÃ‡ÄÄ°Ã–ÅÃœ]{3,5}\b")

def load_state():
    STATE_PATH.parent.mkdir(parents=True, exist_ok=True)
    if STATE_PATH.exists():
        try:
            return set(json.loads(STATE_PATH.read_text()))
        except Exception:
            return set()
    return set()

def save_state(ids):
    STATE_PATH.write_text(json.dumps(sorted(list(ids)), ensure_ascii=False, indent=2))

def sha(text: str) -> str:
    return hashlib.sha256(text.strip().encode("utf-8")).hexdigest()[:24]

def fetch_html():
    r = requests.get(
        FOREKS_URL,
        headers={"User-Agent": USER_AGENT, "Accept-Language": "tr-TR,tr;q=0.9,en;q=0.8"},
        timeout=20,
    )
    r.raise_for_status()
    return r.text

def extract_rows(html: str):
    """
    Foreks sayfasÄ±ndaki kutudaki satÄ±rlarÄ± mÃ¼mkÃ¼n olduÄŸunca dayanÄ±klÄ± ÅŸekilde ayrÄ±ÅŸtÄ±rÄ±r.
    - Her satÄ±rda saÄŸda 'etiket' gibi gÃ¶rÃ¼nen bir A etiketi (ticker) var.
    - SatÄ±r baÅŸlÄ±ÄŸÄ±nÄ± aynÄ± satÄ±rÄ±n ilk linkinden/strong/span'Ä±ndan alÄ±yoruz.
    """
    soup = BeautifulSoup(html, "lxml")

    # Ana liste kartÄ±nÄ± bul (baÅŸlÄ±k 'Piyasa Analizleri' sayfasÄ±ndaki tek bÃ¼yÃ¼k liste)
    # SatÄ±rlar genelde <li> veya <div class="list-item"> benzeri; iki yÃ¶ntemi de deneriz.
    container = None
    for candidate in soup.find_all(["div", "section"]):
        if candidate.get_text(strip=True).startswith("ÅÄ°RKET HABERLERÄ°") or "Åirket Haberleri" in candidate.get_text():
            container = candidate.parent  # satÄ±rlarÄ±n olduÄŸu Ã¼st kapsayÄ±cÄ±
            break
    if container is None:
        # Alternatif: sayfadaki tÃ¼m olasÄ± satÄ±r bloklarÄ±nÄ± tara
        container = soup

    rows = []
    # 1) <li> tabanlÄ± liste
    for li in container.find_all(["li", "div"], recursive=True):
        # SatÄ±rÄ±n iÃ§inde baÅŸlÄ±k olabilecek ilk link/strong/span
        # ve saÄŸda kod olabilecek link (tam bÃ¼yÃ¼k harf 3-5 harf)
        # BaÅŸlÄ±k linki
        title = None
        title_link = None
        # baÅŸlÄ±ÄŸÄ± tutarlÄ± almak iÃ§in en uzun metinli <a>â€™yÄ± seÃ§
        a_tags = [a for a in li.find_all("a", recursive=True) if a.get_text(strip=True)]
        if not a_tags:
            continue
        title_link = max(a_tags, key=lambda a: len(a.get_text(strip=True)))
        title = " ".join(title_link.get_text(" ", strip=True).split())

        # Hisse kodu adaylarÄ±: satÄ±r iÃ§indeki tÃ¼m <a> ve <span> metinlerinde regex aramasÄ±
        codes = []
        for el in li.find_all(["a", "span", "div"], recursive=True):
            text = el.get_text(strip=True)
            # saÄŸdaki etiketler genelde kÄ±sa ve TAM BÃœYÃœK HARF; yabancÄ± kelimeleri elemeye Ã§alÄ±ÅŸ
            for m in TICKER_RE.findall(text):
                # SÄ±k Ã§Ä±kan ama kod olmayan kÄ±saltmalarÄ± ele
                if m in {"TCMB", "CEO", "NVIDIA", "NVDIA", "BIST", "FOREKS"}:
                    continue
                # TÃ¼rkÃ§e bÃ¼yÃ¼k harfleri normalize edip sadece A-Z yapalÄ±m
                norm = (m
                        .replace("Ã‡","C").replace("Ä","G").replace("Ä°","I")
                        .replace("Ã–","O").replace("Å","S").replace("Ãœ","U"))
                if 3 <= len(norm) <= 5 and norm.isupper():
                    codes.append(norm)
        codes = list(dict.fromkeys(codes))  # uniq, sÄ±ra koru

        # Bu satÄ±r bir haber kartÄ± mÄ±? BaÅŸlÄ±kta 'Åirket Haberleri' yazÄ±yorsa atla
        if not title or "ÅÄ°RKET HABERLERÄ°" in title.upper():
            continue

        # EÄŸer hiÃ§ kod yoksa bu haberi tweetlemeyeceÄŸiz
        if not codes:
            continue

        # Ä°lk kodu seÃ§
        ticker = codes[0]
        rows.append({"title": title, "ticker": ticker})

    # DÃ¼ÅŸÃ¼k kaliteli gÃ¼rÃ¼ltÃ¼yÃ¼ ele: Ã§ok kÄ±sa baÅŸlÄ±klar, duyuru olmayanlar
    cleaned = []
    for r in rows:
        if len(r["title"]) >= 20:  # Ã§ok kÄ±sa baÅŸlÄ±klarÄ± at
            cleaned.append(r)
    return cleaned

def compose_tweet(ticker: str, title: str) -> str:
    base = f"ğŸ“° #{ticker} | {title}"
    # KullanÄ±cÄ±nÄ±n 279 karakter sÄ±nÄ±rÄ±
    if len(base) <= 279:
        return base
    # Gerekirse akÄ±llÄ± kes: Ã¶nce parantez/alt aÃ§Ä±klamalarÄ± kÄ±salt
    trimmed = base[:276] + "â€¦"
    return trimmed

def twitter_client():
    api_key = os.getenv("API_KEY")
    api_secret = os.getenv("API_KEY_SECRET")
    access_token = os.getenv("ACCESS_TOKEN")
    access_secret = os.getenv("ACCESS_TOKEN_SECRET")

    auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_secret)
    api = tweepy.API(auth)
    # basit doÄŸrulama
    api.verify_credentials()
    return api

def main():
    print(">> start (Foreks BIST Åirketleri)")
    html = fetch_html()
    rows = extract_rows(html)
    print(f">> parsed rows: {len(rows)}")

    if not rows:
        print(">> no eligible rows")
        return

    seen = load_state()
    api = twitter_client()

    posted_any = False
    # En yeni en Ã¼ste geliyor; sondan baÅŸa gidip eskinin Ã¶nce atÄ±lmasÄ±nÄ± isteyebilirsin.
    for r in rows:
        tweet = compose_tweet(r["ticker"], r["title"])
        uid = sha(tweet)  # aynÄ± tweet bir daha atÄ±lmasÄ±n
        if uid in seen:
            continue
        try:
            api.update_status(status=tweet)
            print(">> tweeted:", tweet)
            seen.add(uid)
            posted_any = True
            time.sleep(3)  # hÄ±z limiti iÃ§in kÃ¼Ã§Ã¼k bekleme
        except Exception as e:
            print("!! tweet error:", e)

    if posted_any:
        save_state(seen)
        print(">> state saved")
    else:
        print(">> nothing new to tweet")

if __name__ == "__main__":
    main()
